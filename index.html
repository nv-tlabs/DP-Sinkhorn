<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!--link rel="icon" href="assets/nvidia.svg"-->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>
      Don't Generate Me: Training Differentially Private Generative Models with
      Sinkhorn Divergence
    </title>
    <style>
      body {
        font-family: "Titillium Web", "HelveticaNeue-Light",
          "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial,
          "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        max-width: 980px;
        text-align: justify;
      }
      .topnav {
        background-color: #eeeeee;
        overflow: hidden;
      }

      .topnav div {
        max-width: 950px;
        margin: 0 auto;
      }

      .topnav a {
        display: inline-block;
        color: black;
        text-align: center;
        vertical-align: middle;
        padding: 14px 16px;
        text-decoration: none;
        font-size: 16px;
      }

      .topnav img {
        width: 100%;
        margin: 0.5em 0px 0.3em 0px;
      }

      h1 {
        font-weight: 300;
        line-height: 1.15em;
        text-align: center;
      }
      .authors a {
        display: inline-block;
        font-size: 20px;
        padding: 15px;
        text-align: center;
      }
      .authors sup {
        color: #313436;
        font-size: 60%;
      }
      .affil li {
        display: inline-block;
        padding: 10px;
        font-weight: 300;
        text-align: center;
      }
      .venue {
        color: #1367a7;
        text-align: center;
      }
      .venue b {
        font-weight: 300;
      }
      .abstract {
        width: auto;
        font-size: 16px;
        color: #666;
        margin-top: 8px;
        margin-bottom: 8px;
      }
      p.abstract {
        width: 80%;
        line-height: 1.6em;
        margin: auto;
      }
      .caption {
        font-size: 14px;
        color: #666;
        text-align: center;
        margin-top: 8px;
        margin-bottom: 8px;
        line-height: 1.25em;
        width: 80%;
        margin-left: auto;
        margin-right: auto;
      }
      
      pre {
        background-color: #eee;
        font-size: 14px;
        white-space: pre;
        padding-left: 20px;
        padding-bottom: 10px;
        margin-top: 0px;
      }
      .section .bibtex {
        margin: 32px 0px 32px 0px;
        display: block;
        font-size: 12pt;
        clear: both;
      }
      .screenshot {
        width: 256px;
        border: 1px solid #ddd;
      }
      .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }
    </style>
  </head>
  <body>
    <div class="topnav" id="myTopnav">
      <div>
        <a href="https://www.nvidia.com/"
          ><img width="100%" src="assets/nvidia.svg"
        /></a>
        <a href="https://nv-tlabs.github.io/"
          ><strong>Toronto AI Lab</strong></a
        >
      </div>
    </div>
    <div class="title">
      <h1>
        Don't Generate Me: Training Differentially Private Generative Models
        with Sinkhorn Divergence
      </h1>
    </div>
    <div class="authors", style="text-align: center;">
      <a href="https://orcid.org/0000-0001-6579-6044" target="_blank">Tianshi Cao<sup>1,2,4</sup></a>
      <a href="" target="_blank">Alex Bie<sup>3</sup></a>
      <a href="http://latentspace.cc/" target="_blank">Arash Vahdat<sup>4</sup></a>
      <a href="https://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler<sup>1,2,4</sup></a>
      <a href="https://nv-tlabs.github.io/author/karsten-kreis/" target="_blank">Karsten Kreis<sup>4</sup></a>
    </div>
    <ul class="affil", style="text-align: center;">
      <li><sup>1</sup>University of Toronto</li>
      <li><sup>2</sup>Vector Institute</li>
      <li><sup>3</sup>University of Waterloo</li>
      <li><sup>4</sup>NVIDIA</li>
    </ul>
    <p class="venue"><b>NeurIPS 2021</b></p>
    <div style="text-align: center; ">
        <a href="assets/DP_sinkhorn-neurips.pdf " style='padding: 15px;' target="_blank">[Paper PDF]</a>
        <a href="assets/DP_Sinkhorn-supplementary.pdf" style='padding: 15px;' target="_blank">[Supplementary Materials]</a>
        <a href="https://arxiv.org/abs/2111.01177" style='padding: 15px;'>[Arxiv]</a>
        [Code] <sup>coming soon</sup>
      
    <section id="teaser">
      <img width="80%" src="assets/motivation.PNG" style="margin-top: 2em;"/>

      <p class="caption">
        We propose DP-Sinkhorn, a method for privacy-preserving generative
        modeling. With DP-Sinkhorn, private data held by data curators can be
        shared to analysts for data-driven downstream tasks while preserving the
        privacy of the data. First, the curator uses DP-Sinkhorn to train a
        privacy-preserving generative model. Then, this model is shared to
        analysts, who sample the model for generated data. The generated data
        is similar to the private data, yet does not reveal details about
        individual data points. Various tasks can be performed on the generated
        data in place of the private data.
      </p>
    </section>
  </div>
    <section id="abstract" style="text-align: center;">
      <h2>Abstract</h2>
      <hr style="width:80%;margin: auto;"/>
      
      <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666; width: 80%; margin: auto; text-align: justify;
      text-align: justify;">
        Although machine learning models trained on massive data have led to
        breakthroughs in several areas, their deployment in privacy-sensitive
        domains remains limited due to restricted access to data. Generative
        models trained with privacy constraints on private data can sidestep
        this challenge, providing indirect access to private data instead. We
        propose DP-Sinkhorn, a novel optimal transport-based generative method
        for learning data distributions from private data with differential
        privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a
        computationally efficient approximation to the exact optimal transport
        distance, between the model and data in a differentially private manner
        and uses a novel technique for controlling the bias-variance trade-off
        of gradient estimates. Unlike existing approaches for training
        differentially private generative models, which are mostly based on
        generative adversarial networks, we do not rely on adversarial
        objectives, which are notoriously difficult to optimize, especially in
        the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn
        is easy to train and deploy. Experimentally, we improve upon the
        state-of-the-art on multiple image modeling benchmarks and show
        differentially private synthesis of informative RGB images.
      </p>
    </section>
    <!-- <section id="paper">
        <h2>Paper and Code</h2>
        <hr />
        <div class="flex-row">
          <div style="box-sizing: border-box; width: 45%; padding: 16px; margin: auto; float: left;">
            <a href="assets/paper_thumbnail.PNG"
              ><img class="screenshot" src="assets/paper_thumbnail.png"
            /></a>
          </div>
          <div style="width: 50%; float: left;">
            <p>
              <b
                >Don't Generate Me:<br />Training Differentially Private
                Generative Models with Sinkhorn Divergence</b
              >
            </p>
            <p>
              Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis
            </p>
            <a href="">Paper PDF</a><br />
            <a href="">Code <sup>Coming soon!</sup></a
            ><br />
            <a href="">Video <sup>Coming soon!</sup></a
            ><br />
          </div>
        </div>
        
      </section> -->
    <section id="motivation" style="text-align: center;">
      <h2>Motivation</h2>
      <hr />
      <div style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">
        <p style="text-indent: 2em">Differential privacy (DP) is a rigorous definition of privacy that quantifies the amount of information leaked by a user, participating in a data release <a href="#References">[1]</a>.
            DP was originally designed for answering queries to statistical databases. In a typical setting, a data analyst (party wanting to use data; e.g. a healthcare company) 
            sends a query to a data curator (party in charge of safekeeping the database; e.g. a hospital), who makes the query on the database and replies with a semi-random answer 
            that preserves privacy. Responding to each new query incurs a privacy cost. If the analyst has multiple queries, the curator must subdivide the privacy budget to spend on each query.
            Once the budget is depleted, the curator can no longer respond to queries, preventing the analyst from performing new, unanticipated tasks with the database.
            To sidestep this challenge, generative models can be applied as a general and flexible data-sharing medium: The curator trains a generative model with DP guarantees, and releases the model to analysts. 
            The analysts then generate data from this model, which can be used for any downstream task.
            In this work, our goal is to <em>learn a generative model while satisfying the constraints of differential privacy</em>.  
        </p>
        <p style="text-indent: 2em">
            Differentially private learning of generative models has been studied mostly using generative adversarial networks (GANs). While GANs in the non-private setting can synthesize complex data such as high definition images,
            their application in the private setting is challenging. This is in part because GANs suffer from training instabilities, which can be exacerbated by adding noise to the network's gradients during training, a common technique to implement DP.
            Hence, GANs typically require careful hyperparameter tuning and supervision during training to avoid collapsing. This goes against the principle of privacy, where repeated interactions with data need to be avoided. To overcome these issues, we 
            propose a non-adversarial generative learning approach that enjoys <em>stabler convergence</em>, produces <em>higher quality outputs</em>, and is <em>more robust to the choice of hyperparameters</em>.
            
        </p>   
      </div>
    </section>
    <section id="method overview" style="text-align: center;">
      <h2>Method Overview</h2>
      <hr />
      <div style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">
        <p style="text-indent: 2em">We propose <em>DP-Sinkhorn</em>, a novel method to train differentially private generative
            models using a semi-debiased Sinkhorn loss. DP-Sinkhorn is based on the framework
             of optimal transport (OT), where the problem of learning a generative model is 
             framed as minimizing the optimal transport distance, a type of Wasserstein 
             distance, between the generator-induced distribution and the real data distribution.
             The optimal transport distance has many desirable properties as an objective function, but they are difficult to compute due to the optimization problem on the transport plan.
             The entropy regularized Wasserstein distance <a href="#References">[2]</a> makes computation of the transport plan tractable at the cost of introducing bias:
            </p>
            <p style="text-align: center;">
             \(W_{c,\lambda}(\mu, \nu)= \min_{\pi \in \Pi} \int c({x},{y})d\pi({x},{y}) + \lambda \int \log\left( \frac{d\pi({x},{y})}{d\mu({x})d\nu({y})}\right)d\pi({x},{y}), \)
             </p>
             <p>where \(\pi(x,y)\) is the transport plan defined as \(\Pi = \{ \pi(x,y) \in \mathcal{P}(\mathcal{X} \times \mathcal{X})| \int \pi(x, \cdot) d x = \nu, \int \pi(\cdot, y) d y  = \mu\}\), with cost function \(c(x,y)\), and regularization weight parameter \(\lambda\).</p>
             <p style="text-indent: 2em">
             The Sinkhorn divergence uses auto-correlation terms to reduce the entropic bias introduced by ERWD with respect to the exact Wasserstein distance <a href="#References">[3]</a>. 
             Empirically computing the Sinkhorn divergence exhibits a bias-variance trade-off. One option is to compute it with a single batch of real data \(X\) and a single batch of generated data \(Y\) as: 
             <p style="text-align: center;">\(\hat{S}_{c, \lambda}(X, Y) = 2 \hat{W}_\lambda (X, Y) -\hat{W}_\lambda (X, X) - \hat{W}_\lambda (Y, Y).\)</p>
            <p>
             This is a biased estimator as it under-estimates the magnitudes of the two 
             auto-correlation terms. Alternatively, if we independently draw two batches of real and generated data, we obtain an unbiased estimator:
            </p>
             <p style="text-align: center;">
                \(\hat{S}_{c, \lambda}(X, Y, X', Y') =  2\hat{W}_\lambda (X, Y) - \hat{W}_\lambda (X, X') - \hat{W}_\lambda (Y, Y').\)
             </p> 
             <p style="text-indent: 2em">While unbiased, this formulation adds variance to the mini-batched gradients which can be harmful to training. Note that the last term in these two equations affects solely the real data, which is not a function of the generator. Hence, this term has no impact on the learning problem.
                 We design a semi-debiased formulation that controls the bias-variance tradeoff of Sinkhorn divergence. Specifically, the semi-debiased Sinkhorn loss partially re-samples the first batch to create the second batch.
                 The amount of variance can be controlled by adjusting the portion of the batch that is re-sampled. The semi-debiased Sinkhorn loss is formally defined as below.
                </p>
            <div style="font-style: italic; width: 90%; margin: auto;"> <b>Definition</b>
                (Semi-debiased Sinkhorn Loss) For a mixture fraction \(p \in [0,1]\) and natural number \(n\), 
                \(n' = floor(n \times p)\). Given \(n+n'\) generated samples \(X \in \mathcal{X}^{n+n'}\) and \(m\) real samples \(Y \in \mathcal{X}^{m}\), the semi-debiased Sinkhorn loss is defined as:
            </div>
             <p style="text-align: center; width: 90%; margin: auto;">
                \(\hat{S}_{c, \lambda, p}(X, Y) = 2 \hat{W}_\lambda (X^{[0:n]}, Y) -\hat{W}_\lambda (X^{[0:n]}, X^{[n':n+n']})\)
            </p>
             
            <p>
                This semi-debiased loss can be viewed as interpolating between the biased Sinkhorn loss (\(p=0\)) and the fully unbiased Sinkhorn loss  (\(p=1\)).
            </p>
            <img style="width: 90%;"
            src="assets/sinkhorn dp diagram 2.png"
            alt="Diagram of Semi-debiased DP-Sinkhorn"
            />
            <p class="caption">Flow diagram of DP-Sinkhorn for a single training iteration</p>
            <p style="text-indent: 2em">
              We use the semi-debiased Sinkhorn loss to train generative models. In each iteration, we first generate synthetic data and sample real data, with the synthetic data split into two batches to perform debiasing. 
              An element-wise cost function that measures the difference between pairs of examples is applied on each element of synthetic and real data to form the cost matrix. 
              Then, the semi-debiasing Sinkhorn divergence is calculated on these batches. 
              Privacy protection is achieved by applying the Gaussian mechanism (a well-known method for adding noise to achieve DP) to gradients of the Sinkhorn loss w.r.t generated images.
              </p>
     </div>
    </section>
        
      
      
    
    <section id="results" style="text-align: center;">
      <h2>Experimental Results</h2>
      <hr />
      <div class="flex-row">
        <img
        src="assets/dp_sinkhorn_mnist_m1.png"
        alt="Generated images on MNIST"
        style="width: 49%"
      />
      <img
        src="assets/dp_sinkhorn_fashion_m1.png"
        alt="Generated images on FashionMNIST"
        style="width: 49%"
      />
      </div>
      
      <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666; text-indent: 2em;">We evaluate DP-Sinkhorn on popular benchmarks of MNIST and Fashion MNIST for privacy-preserving generative modelling. Above are synthetic images produced by DP-Sinkhorn using a privacy budget of (\(10, 10^{-5}\))-DP.
          Images are generated with class conditioning. To measure the utility of generated images, we train a classifier on synthetic data, and evaluate it on held-out real data. 
        DP-Sinkhorn achieves state-of-the-art performance in both image quality and utility among differentially private generative models.
      </p>
      <img
        src="assets/celebA.PNG"
        alt="Generated images on CelebA"
        style="width: 60%"
      />
      <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666; text-indent: 2em;">
          We also experiment with DP-Sinkhorn on RGB images. Here, we use CelebA downsampled to 32x32 as real data, and consider the task of gender conditional generation. DP-Sinkhorn is able to produce informative synthetic data for downstream gender classification despite the added image complexity. Images generated by DP-Sinkhorn resemble blurry faces, while existing baselines cannot produce data with reasonable appearance.
      </p>

      
    </section>
   

    <section id="bibtex" style="text-align: center;">
      <h2>Citation</h2>
      <hr />
      <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">For feedback and questions please reach out to <a href="mailto:tianshic@nvidia.com">Tianshi Cao</a> and <a href="mailto:kkreis@nvidia.com">Karsten Kreis</a>. If you find this work useful for your research, please consider citing it as:</p>
      <pre><code, style="text-align: left;display: inline-block;">@inproceedings{Cao2021DPSinkhorn,
title = {Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence}, 
author = {Tianshi Cao and Alex Bie and Arash Vahdat and Sanja Fidler and Karsten Kreis},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2021}
}
</code></pre>
<a name="References"></section id="Reference" style="text-align: center;"></a>
    <section>
      <hr>
      <h3>References</h3>
      <p style="line-height: 1.5em; text-align: left; font-size: 12pt; color: #666;">
        [1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in Theoretical Computer Science, vol. 9, p. 211–407, Aug. 2014. <br>
        [2] G. Peyr&eacute; and M. Cuturi, “Computational Optimal Transport,” Foundations and Trends in Machine Learning, vol. 11, no. 5-6, pp. 355–607, 2019. <br>
        [3] J. Feydy, T. S&eacute;journ&eacute;,  F.-X. Vialard,  S.-i. Amari, A. Trouv&eacute;, and G. Peyr&eacute;, “Interpolating between optimal transport and MMD using sinkhorn divergences,” in The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681–2690, 2019.
      </p>
    </section>
  </body>
</html>
